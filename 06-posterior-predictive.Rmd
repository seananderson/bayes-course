---
title: "Posterior predictive checking exercise"
output:
  html_document:
    toc: true
    toc_float: true
---

# Goals

- Experiment with using posterior predictive checks to discover issues with a Bayesian probability model.

# Setup

I've simulated and fit models to 3 data sets. Each model has some issue and the probability model does not represent the data well. See if you can figure out what the problem is by using posterior protective checks. 

For each exercise, you'll read in two objects:
- `df`: a data frame with a predictor `x` and observed values `y`. In some cases there may be an additional column defining groups `g`.
- `yrep`: a matrix of posterior predictive simulated observations returned by `brms::posterior_predict()`. There are 20 rows (20 samples) by 100 or 200 columns (number of data points). 

Use `bayesplot::ppc_*` functions (or make the plots yourself) to find the issues. I've included the code here so you can focus on the interpretation.

Answer the following questions:

1. What is the visualization showing?
2. Do the predictive simulations have similar properties to the observed data? I.e., could the model have generated the observed data? 
3. If not, what is the issue, what might have caused it, and how you might you fix it? (You don't actually have to fix it for this exercise!)

# Exercise 1

```{r}
df <- readRDS(here::here("data/ppcheck-df1.rds"))
yrep <- readRDS(here::here("data/ppcheck-yrep1.rds"))
```

```{r}
y <- df$y
bayesplot::ppc_dens_overlay(y, yrep)
bayesplot::ppc_error_scatter_avg_vs_x(y, yrep, df$x)
bayesplot::ppc_intervals(y, yrep, x = df$x)

par(mfrow = c(1, 3))
plot(df$x, y)
plot(df$x, yrep[1,])
plot(df$x, yrep[2,])
```

Answer: the model isn't creating the curvature in the values of `y` with respect to `x`. The model was fit as `y ~ x` but is missing a quadratic term: `y ~ x + I(x^2)` or `y ~ x + poly(x, 2)`. <!-- exercise -->

# Exercise 2

```{r}
df <- readRDS(here::here("data/ppcheck-df2.rds"))
yrep <- readRDS(here::here("data/ppcheck-yrep2.rds"))
```

```{r}
y <- df$y
par(mfrow = c(1, 3))
plot(df$x, df$y)
plot(df$x, yrep[1,])
plot(df$x, yrep[2,])
bayesplot::ppc_dens_overlay(y, yrep)
```

Answer: the model is isn't creating enough spread in the data for larger values of `x`. The observation error distribution assumptions look off. These data were generated from a negative binomial model but were fit with a Poisson likelihood.  <!-- exercise -->

# Exercise 3

```{r}
df <- readRDS(here::here("data/ppcheck-df3.rds"))
yrep <- readRDS(here::here("data/ppcheck-yrep3.rds"))
```

Hint: try visualizing the posterior predictive simulations grouped or coloured by column `g`.

```{r}
y <- df$y
par(mfrow = c(1, 3))
plot(df$x, df$y)
plot(df$x, yrep[1,])
plot(df$x, yrep[2,])

bayesplot::ppc_dens_overlay(y, yrep)
bayesplot::ppc_dens_overlay_grouped(y, yrep, df$g)

df$yrep1 <- yrep[1,]

library(ggplot2)
ggplot(df, aes(x, y, colour = g)) + geom_point()
ggplot(df, aes(x, yrep2, colour = g)) + geom_point()
```

Answer: the model is lacking random intercept by group `g`. The posterior simulations therefore lack the clumping of observations seen in the observed data.  <!-- exercise -->

