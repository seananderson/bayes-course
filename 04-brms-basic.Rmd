---
title: "An introduction to applied Bayesian regression using brms"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.asp = 0.618,
  fig.align = "center"
)
```

# Objectives

- Learn to fit pre-packaged Bayesian regression models with brms.
- Become familiar with the concepts of posterior predictive checking and 
  manipulating posterior samples to calculate posterior probabilities.
  
# Setup

Let's load dplyr, ggplot2, and brms 

```{r, warning=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(brms)
theme_set(theme_light())
dir.create("cache", showWarnings = FALSE)
options(brms.file_refit = "on_change") # re-fit cached models if changes
```

Any time we use rstan (or a package that relies on rstan, such as brms or rstanarm), we can set an R option to use parallel processing with all available cores: `options(mc.cores = parallel::detectCores())`. This example should run so quickly that it will likely run faster on a single core, so you may choose to skip this or explicitly set it to 1 core.

```{r, eval=FALSE}
# options(mc.cores = parallel::detectCores())
# options(mc.cores = 1)
```

# Data

We are going to work with data from:

Hughes, B.B., Lummis, S.C., Anderson, S.C., and Kroeker, K.J. 2018. Unexpected resilience of a seagrass system exposed to global stressors. Glob. Change Biol. 24(1): 224â€“234. <https://doi.org/10.1111/gcb.13854>

The data come from a mesocosm experiment by Brent Hughes where he manipulated water pH and whether or not nutrients were added (to represent nutrient loads in eelgrass beds) to 14 200L barrels. He measured a number of variables, but the response variable we are going to work with here is the increase in mass of seahares (*Phyllaplysia taylori*), a type of sea slug, after 24 days.

```{r}
d <- readRDS("data/hughes-etal-2018.rds") %>%
  filter(label == "Change in seahare mass (g FW)") %>% 
  rename(change_seahare_mass_g_fw = value) %>% 
  select(-label, -figure_panel, -nutrients_text, -response)
glimpse(d)
```

```{r}
ggplot(d, 
  aes(ph, change_seahare_mass_g_fw, colour = as.factor(nutrients))) + 
  geom_point()
```

Let's rescale (center and possibly divide by 2 SDs) the predictors. This is important

1. so that we have some idea of what reasonable prior values will be,
2. so that our coefficients are on a reasonable scale for interpretation and 
   for Stan, and
3. so that we can add a quadratic effect and have one coefficient represent the
   slope and the other the curvature.

```{r}
d <- mutate(d,
  ph_scaled = as.numeric(scale(ph))
)
```

Let's look at the data:

```{r}
ggplot(d, 
  aes(ph_scaled, change_seahare_mass_g_fw, 
    colour = as.factor(nutrients))) + 
  geom_point()
```

# Fitting a model

We are going to fit this model with the `brms::brm()` function.

```{r, results='hide', warning=FALSE}
fit <- brm(
  log(change_seahare_mass_g_fw) ~ ph_scaled + I(ph_scaled^2) + nutrients,
  data = d, 
  iter = 2000, 
  chains = 4,
  file = "cache/seahare",
  prior = c(
      set_prior("normal(0, 5)", class = "b"),
      set_prior("normal(0, 10)", class = "Intercept"),
      set_prior("student_t(3, 0, 3)", class = "sigma")
    )
)
```

There are a variety of functions available to inspect our model including the usual print or summary function:

```{r}
summary(fit)
```

Take a look at the output and make sure you understand everything there.

## Questions:

Open the help file `?summary.brmsfit` and answer the following questions:

1. What does the `Estimate` column represent here? Hint: read `?summary.brmsfit`.
2. What does the `Est.Error` column represent here?
3. What is `sigma` here?
4. Do the `Rhat` and `ESS` columns look reasonable?

Bonus questions:

5. Can you get `summary.brmsfit()` to return medians? What does the `Est.Error` column now mean?
6. Can you get `summary.brmsfit()` to return information on the priors?
7. Can you get `summary.brmsfit()` to return 87% CIs?

```{r}
summary(fit, robust = TRUE)
summary(fit, priors = TRUE)
summary(fit, prob = 0.87)
```

There are a lot of helper functions in brms to explore. Here are some useful ones:

```{r}
brms::prior_summary(fit)
# brms::conditional_effects(fit)
# brms::posterior_linpred(fit)
# brms::posterior_predict(fit)
# brms::as_draws(fit)
brms::stancode(fit)
brms::standata(fit)
# brms::LOO(fit)
```

# Inspecting the chains for convergence

We are going to use the plotting functions from the package bayesplot, which is also developed by the Stan developers. These plotting functions will work with any kind of MCMC output, not just the output from brms, rstanarm, or rstan, as long as you format the samples correctly.

There are many available plotting functions in the bayesplot package. Before we start exploring them, we need to make sure that our chains are consistent with convergence. To start with we already checked the effective sample size and Rhat values, but there's no substitute for visually inspecting the chains!

The primary way of looking at MCMC chains is as overlaid time series:

```{r}
bayesplot::mcmc_trace(fit)
```

How does that look to you? 

Another thing to check is the autocorrelation in the chains:

```{r}
bayesplot::mcmc_acf(fit)
```

## Question:

1. Is autocorrelation in the chains a problem in itself?

# Posterior predictive checks

Posterior predictive checking is a powerful concept in Bayesian statistics. 

The basic idea is to simulate new observation from the model several times and then compare those simulated data sets to the data that we observed. We can then slice and dice that comparison creatively to make sure that our Bayesian probability model is a good representation of the process that generated the observed data. They should be indistinguishable in any way you can think of to compare them.

We could do this manually, although the bayesplot package has a large number of helpful plots already in available. We will use the built-in `pp_check()` shortcuts for the rest of this exercise, but know that these are just calling the bayesplot functions, and you can use the bayesplot functions with MCMC output from any Bayesian models sampled with MCMC methods.

Here are all the available posterior predictive checking functions in the bayesplot package:

```{r}
bayesplot::available_ppc()
```

brms can call these posterior predictive functions directly, although it will generate new simulations each time. E.g.:

```{r}
brms::pp_check(fit, ndraws = 50)
```

## Question:

1. What are we looking at here?
2. Are the draws from the posterior consistent with the data that we observed?

To speed things up, we can instead take one set of posterior predictive draws and then we can plot them in various ways using Bayesplot. E.g.

```{r}
y <- log(d$change_seahare_mass_g_fw)
yrep <- posterior_predict(fit, ndraws = 50)
bayesplot::ppc_dens_overlay(y, yrep)
```

Is the same as:

```{r}
brms::pp_check(fit, ndraws = 50, type = "dens_overlay")
```

Where we found `dens_overlay` by running `bayesplot::available_ppc()` and removing the `ppc_` part.

Read about the various available plotting functions at:

```{r, eval=FALSE}
?bayesplot::`PPC-overview`
```

### Your turn

Experiment with the available posterior predictive checking functions to evaluate our model. 

```{r}
pp_check(fit, type = "hist") # exercise
pp_check(fit, type = "error_scatter") # exercise
pp_check(fit, type = "scatter") # exercise
pp_check(fit, type = "scatter_avg") # exercise
pp_check(fit, type = "scatter_avg_grouped", group = "nutrients") # exercise
pp_check(fit, type = "ecdf_overlay") # exercise
pp_check(fit, type = "intervals") # exercise
pp_check(fit, type = "intervals", x = "change_seahare_mass_g_fw") # exercise
pp_check(fit, type = "intervals", x = "nutrients") # exercise
pp_check(fit, type = "intervals", x = "ph_scaled") # exercise
```

# Summarizing the posterior samples graphically

Again, we can look at trace plots like this:

```{r}
bayesplot::mcmc_trace(fit)
```

These are the available plotting functions:

```{r}
bayesplot::available_mcmc()
```

### Your turn

Experiment with the available plotting functions to summarize the posterior probabilities of the parameters in our model. Which do you find most useful here? 

```{r}
bayesplot::mcmc_areas(fit) # exercise
bayesplot::mcmc_intervals(fit) # exercise
bayesplot::mcmc_combo(fit) # exercise
bayesplot::mcmc_areas_ridges(fit) # exercise
```

- What does the `(Intercept)` coefficient represent?
- What does the `nutrients` coefficient represent?
- What does the `ph_scaled` coefficient represent? 
- What does the `I(ph_scaled^2)` coefficient represent?
- What does the `sigma` coefficient represent?

Think back to our introduction to Bayesian statistics. How do we interpret these summaries of the coefficients probabilistically?

Note that we can easily extract the credible intervals with:

```{r}
posterior_interval(fit)
posterior_interval(fit, prob = 0.9)
posterior_interval(fit, prob = 0.89) # see 'Statistical Rethinking'
posterior_interval(fit, prob = 0.5)
```

Why might we prefer 90% or 50% or even 89% credible intervals over the usual 95%?

# Checking the priors

We are going to talk about priors more extensively soon.

It's helpful to know that you can extract details on the priors from an brms model with the `prior_summary()` function. In this case we specified all the priors explicitly, *which is a good practice*. This function is a good way to check that the priors were interpreted correctly, and is also a good way to discover parameters that you might have forgot to set the priors on explicitly.

```{r}
brms::prior_summary(fit)
```

Let's compare the full posterior distribution for a parameter to its prior as an example.

For models fit with brms, we can extract the posterior samples with `as.data.frame()` or `as.matrix()`. Let's use the data frame version. We'll also convert to a tibble, just so that it prints nicely.

```{r}
post <- as_tibble(as.data.frame(fit))
post
```

What does each column in this data frame represent?

Our prior on ph_scaled^2:

```{r}
prior <- tibble(
  b_ph_scaled = seq(-10, 10, length.out = 300),
  density = dnorm(b_ph_scaled, 0, 3)
)
prior
```

Plot them both:

```{r}
# note `after_stat(density)` to get probability densities not counts
# see `?geom_histogram()`
ggplot() +
  geom_histogram(data = post, aes(b_ph_scaled, after_stat(density)), 
    bins = 80) +
  geom_ribbon(data = prior, aes(x = b_ph_scaled, 
    ymax = density, ymin = 0), fill = "blue", alpha = 0.5) +
  coord_cartesian(xlim = c(-5, 2.5)) +
  coord_cartesian(expand = FALSE) # no gap below 0
```

One thing we haven't done is test the sensitivity of our posterior samples to the choice of priors. How could we go about testing that?

# Shiny Stan

The shinystan package is a one-stop shop for inspecting a Stan model. For a model fit with brms, rstanarm, or rstan we can launch it with:

```{r, eval=FALSE}
launch_shinystan(fit)
```

# Plotting the posterior distribution of the linear predictor

```{r}
newdata <- expand.grid(
  ph_scaled = seq(min(d$ph_scaled), max(d$ph_scaled), length.out = 500),
  nutrients = c(0, 1)
)
head(newdata)
```

We can extract samples from the linear predictor with the `posterior_linpred()` function. These are samples from the posterior without observation error. In other words, these are similar in concept to the confidence interval you would get out of `predict.glm()` or `predict.lm()`.

```{r}
posterior_linear <- posterior_epred(fit, newdata = newdata)
dim(posterior_linear)
```

So we now have a matrix that is 4000 rows long and 1000 columns wide. Where do the 4000 and 1000 come from?

We can summarize the samples however we would like. I'm going to suggest we use the median and the 25% and 75% quantiles. We could also choose to use the mean and any other quantiles we wanted.

```{r}
newdata$est <- apply(posterior_linear, 2, median)
newdata$lwr <- apply(posterior_linear, 2, quantile, probs = 0.25)
newdata$upr <- apply(posterior_linear, 2, quantile, probs = 0.75)
```

```{r}
pp <- posterior_predict(fit, newdata = newdata)
newdata$lwr_pp <- apply(pp, 2, quantile, probs = 0.25)
newdata$upr_pp <- apply(pp, 2, quantile, probs = 0.75)

ggplot(newdata, aes(ph_scaled, exp(est),
  group = nutrients, ymin = exp(lwr), ymax = exp(upr),
  fill = as.factor(nutrients))) +
  geom_ribbon(alpha = 0.2) +
  geom_ribbon(alpha = 0.2, aes(ymin = exp(lwr_pp), ymax = exp(upr_pp))) +
  geom_line(lwd = 1, aes(colour = as.factor(nutrients))) +
  geom_point(data = d, aes(ph_scaled, change_seahare_mass_g_fw,
    colour = as.factor(nutrients)), inherit.aes = FALSE) +
  ylab("Change in seahare mass (g FW)")
```

Note that I exponentiated the predictions to make our plot on the original natural scale.

# Summarizing the posterior distribution multiple ways

One of the nice things about Bayesian statistical models is that we can quantify the probability of nearly any comparison we can imagine. All you have to do is add, subtract, multiply, or divide the samples. Let's try some examples.

As a reminder, `post` comes from:

```{r}
post <- as_tibble(as.data.frame(fit))
post
```

What if we wanted to know the probability that there is a negative (frowning) quadratic shape (vs. a positive (smiling) quadratic shape) to the relationship? We can get that from the ph^2 term since we centered our ph predictor before fitting.

```{r}
ph2_samples <- post$b_ph_scaled
mean(ph2_samples < 0)
mean(ph2_samples > 0)
```

We're taking advantage of the fact that R treats `TRUE` and `FALSE` as 1 and 0. So by taking the mean, we are doing the same thing as:

```{r}
sum(ph2_samples < 0) / length(ph2_samples)
```

What is the probability that the change in seahare mass is greater in the case where nutrients were not added? 

```{r}
mean(post$b_nutrients < 0)
```

A major benefit to MCMC sampling of Bayesian models is how easy it is to quantify any comparison you want to make. 

For example, how much greater would you expect the change in seahare mass to be under conditions of the lowest pH tested with nutrients compared to the average pH condition without nutrients?

I.e. compare the blue posterior in the lower left to the pink posterior in the middle of the last plot. Let's compare the expectations not the ratio of new observations including their observation error (i.e. the linear predictions not the full "posterior predictions").

```{r}
min_ph <- min(d$ph_scaled)
mean_ph <- mean(d$ph_scaled)

condition1 <- data.frame(
  ph_scaled = min_ph,
  nutrients = c(0))
pp1 <- posterior_linpred(fit, newdata = condition1)[,1]

condition2 <- data.frame(
  ph_scaled = mean_ph,
  nutrients = c(0))
pp2 <- posterior_linpred(fit, newdata = condition2)[,1]

ratio <- exp(pp2) / exp(pp1)
ggplot(tibble(ratio = ratio), aes(ratio)) + 
  geom_histogram() +
  scale_x_log10() +
  geom_vline(xintercept = 1)

quantile(ratio, probs = c(0.11, 0.5, 0.89))
mean(ratio > 1)
```

What's the probabilty this ratio is greater than 3?

```{r}
mean(ratio > 3)
```

If you can think it you can quantify it. And all you have to do is manipulate the MCMC samples. Add, subtract, multiply, or divide as needed.
