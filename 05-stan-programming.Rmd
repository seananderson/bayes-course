---
title: "An introduction to Stan programming"
author: "Sean Anderson"
output:
  html_document:
    toc: true
    toc_float: true
---

# Goals 

- Learn the basics of Stan model syntax and how to interact with Stan in R
- Learn to generate our own posterior predictions from a Stan model 
- Learn to calculate LOOIC from a Stan model we wrote ourselves

# Setup 

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.asp = 0.618,
  fig.align = "center"
)
show_file <- function(file) {
  cat(paste(readLines(file), collapse = "\n"))
}
```

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
theme_set(theme_light())
```

Let's load the rstan R package, which lets us interact with R.

```{r}
library(rstan)
```

If we wanted, we could set our option to use parallel processing. This would be useful with more complicated models or more data. For this simple exercise, it will probably be fastest with a single core. 

```{r}
# options(mc.cores = parallel::detectCores())
options(mc.cores = 1)
```

Another important option we can set with rstan is whether we want the compiled model to be saved so that we don't have to recompile it every time we run it. You probably want to set this because the compilation can take a while. 

```{r}
rstan_options(auto_write = TRUE)
```

# Simulating data

Let's simulate some data to fit a very simple Stan model to. We will focus on a linear model with normally distributed errors.

```{r}
set.seed(42)
N <- 30
x <- rnorm(N, 0, 0.5)
alpha <- -0.2
beta <- 0.4
sigma <- 0.3
y <- rnorm(N, mean = alpha + x * beta, sd = sigma)
dat <- tibble(x = x, y = y)
```

```{r}
ggplot(dat, aes(x, y)) + geom_point()
```

# The Stan model 

Take a look at the following Stan model that I have set up. 

We'll talk about the various sections as a group. 

```{r}
show_file("stan/lm-simple.stan")
```

This next version also has a generated quantity section. This is the one we will actually run. The generated quantities make it easier to do some things after like make posterior predictions. 

```{r}
show_file("stan/lm.stan")
```

# Fitting the model

The first time we run the next code chunk, Stan will translate our model into C++ and compile it. This will take little while. After that, assuming we set `rstan_options(auto_write = TRUE)`, Stan will avoid recompiling the model and less something in the model code changes.

Let's sample from the model now:

```{r, message=FALSE, results='hide'}
fit <- stan("stan/lm.stan", chains = 4, iter = 2000,
  data = list(x = dat$x, y = dat$y, N = length(dat$y)))
```

Congratulations --- you fit your first handwritten Stan model! We can do everything with the posterior samples that we could do with the samples from an rstanarm model or a brms model.

Some of the built-in helper functions and those packages won't work with our model though.

```{r, eval=FALSE}
fit
```

Notice all of the extra lines of reported results for our generated quantities? We can focus just on the parameters we want with:

```{r}
pars <- c("alpha", "beta", "sigma")
print(fit, pars = pars)
```

We can use shinystan if we want to look at the model. 

```{r, eval=FALSE}
shinystan::launch_shinystan(fit)
```

The default plot method shows point estimates and credible intervals.

```{r}
plot(fit, pars = pars)
```

There are other options. 

```{r, eval=FALSE}
?rstan::`rstan-plotting-functions`
stan_dens(fit, pars = pars)
```

Alternatively, and probably preferably, we can use the bayesplot package. 

Experiment with inspecting the posterior chains using the bayesplot package.

```{r}
fit_array <- as.array(fit)
bayesplot::mcmc_trace(fit_array, pars = pars) 
bayesplot::mcmc_dens_overlay(fit_array, pars = pars) # exercise
```

Experiment with inspecting the posterior predictive distribution using the bayesplot package:

```{r}
pp <- extract(fit)$posterior_predictions
bayesplot::ppc_dens_overlay(y = dat$y, yrep = pp[1:25, ])
```

# Manipulating the posterior samples ourselves

```{r}
post <- rstan::extract(fit)
```

The output from `rstan::extract()` is a named list. Each element of the list is a numeric vector of samples if that parameter had one dimension (as all of our parameters did this time), or a matrix of samples if, say, beta had represented multiple slope parameters. 

```{r}
names(post)
dim(post$beta)
```

Let's overlay some model fits from the posterior on the data:

```{r}
N <- 100
ggplot(dat, aes(x, y)) + geom_point() +
  geom_abline(
    intercept = post$alpha[1:N], 
    slope = post$beta[1:N], 
    alpha = 0.2)
```

Is that starting to look like the confidence/credible intervals you are used to looking at?

What if we wanted to look at posterior predictions? We can do that with the `posterior_predictions` samples that we created from the `generated quantities` section of our Stan model. We did that above. Let's do it in R to see how we could create them after.

In other words, how would we add on the observation component to our predictions? Let's manually create 8 draws from the posterior predictive distribution and compare them to our data. 

There are many ways you could do this. The following is one way. We will create a list with posterior predictions within data frames and then bind the elements of the list into a big data frame. We are doing this so it is easy to plot the output in ggplot.

```{r}
n_sim <- 8
out <- vector(mode = "list", length = 8)
for (i in seq_along(out)) {
  out[[i]] <- tibble(x = x)
  out[[i]]$i <- i
  out[[i]]$y_pp <- 
    rnorm(
      n = length(x), 
      mean = post$alpha[i] + post$beta[i] * x, 
      sd = post$sigma[i])
}
out <- dplyr::bind_rows(out) # turn the list into a data.frame
out$type <- "posterior prediction"

# add the real data as the last panel:
out <- dplyr::bind_rows(out, 
  tibble(x = x, y_pp = y, type = "observed", i = 9))

ggplot(out, aes(x, y_pp, colour = type)) + geom_point() + 
  facet_wrap(~i)
```

# Model comparison 

All we need to do to get at the leave-one-out information criterion is to include the `log_lik` variable in the generated quantities section. Then we can calculate LOO with:

```{r}
log_lik <- loo::extract_log_lik(fit, merge_chains = FALSE)
rel_n_eff <- loo::relative_eff(exp(log_lik))
loo::loo(log_lik, r_eff = rel_n_eff)
```

Of course, LOO isn't very useful unless we have another model to compare this one with. 
