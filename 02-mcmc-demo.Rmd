---
title: "The Metropolis MCMC algorithm in R"
author: "Sean Anderson"
output:
  html_document:
    toc: true
    toc_float: true
---

# Goals 

- Demystify MCMC sampling by coding one ourselves in R and manually updating it
- Gain an intuition for the need to tune MCMC algorithms with interactive visualization

# Setup and data simulation

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.asp = 0.618,
  fig.align = "center"
)
```

Run this next code chunk once to generate our data.

We are are simulating data with a given mean and standard deviation. We'll effectively be estimating an intercept-only model.

```{r}
set.seed(123)
mu <- 1
sigma <- 2
dat <- rnorm(50, mean = mu, sd = sigma) # our simulated data
plot(dat)
```

Then this chunk will create a blank plot that we will fill in with our MCMC samples.

```{r}
plot(1, xlim = c(0, 30), ylim = c(-2, 2), type = "n", ylab = expression(mu), 
  xlab = "Chain iteration")
jump_sd <- 1 # our chosen jumping standard deviation
i <- 1 # starting iteration
previous_proposal <- 0 # initial proposal value
set.seed(123)
```

# Manually running our MCMC algorithm

Run this next code chunk repeatedly; each MCMC sample will get added to the open plot.

Things to be aware of:
- We're going to chose a prior of Normal(0, 5^2), i.e. `dnorm(mean = 0, sd = 5)`.
- We're going to assume we know the error standard deviation `sigma` so we can keep this example simple and have only one parameter.

```{r, eval=FALSE}
(proposal <- rnorm(1, previous_proposal, jump_sd))
# ----------------------------------------------------------------

(log_like_proposal <- sum(dnorm(dat, mean = proposal, sd = sigma, log = TRUE)))
(log_like_previous <- sum(dnorm(dat, mean = previous_proposal, sd = sigma, log = TRUE)))
# ----------------------------------------------------------------

(log_like_prior_proposal <- dnorm(proposal, mean = 0, sd = 5, log = TRUE))
(log_like_prior_previous <- dnorm(previous_proposal, mean = 0, sd = 5, log = TRUE))
# ----------------------------------------------------------------

# Combine the log-prior with the log-likelihood to get a value that is
# proportional to the posterior probability of that parameter value given the
# data:
(log_posterior_proposal <- log_like_prior_proposal + log_like_proposal)
(log_posterior_previous <- log_like_prior_previous + log_like_previous)
# ----------------------------------------------------------------

# Calculate the ratio of the proposal and previous probabilities:
# (prob_ratio <- exp(log_posterior_proposal) / exp(log_posterior_previous))
(prob_ratio <- exp(log_posterior_proposal - log_posterior_previous))
# ----------------------------------------------------------------

# If the probability ratio is > 1, then always accept the new parameter values.
# If the probability ratio is < 1, then accept the new parameter values in
# proportion to the ratio.
if (runif(1) < prob_ratio) {
  print("Accept new proposal")
  (previous_proposal <- proposal)
} else {
  print("Retain previous value")
  (previous_proposal <- previous_proposal)
}
# ----------------------------------------------------------------

points(i, previous_proposal)
i <- i + 1 # update counter for next time
```

You now have an MCMC chain! The distribution of those values (if you take enough of them) will reflect the distribution of the parameter(s).

```{r, eval=FALSE}
abline(h = mu, lty = 2) # our known true value
```

# MCMC tuning demo

The following code chunk contains the same code but embedded in a function so that we can call it repeatedly with different argument values. 

```{r}
mcmc_example <- function(
  mu = 4, # the true mean; we will estimate this
  sigma = 2, # the true residual SD; we will assume we know this for simplicity
  .n = 30, # number of data points to simulate
  prior_mu = 0, # our prior distribution mean
  prior_mu_sd = 10, # our prior distribution SD on the mean
  # Next, the SD of our jump function.
  # Too small a value and the chain might take a
  # very long time to get to the right parameter space or might get stuck in the
  # wrong area of the parameter space.
  # Too large a value and the proposed values will be often rejected, and again,
  # the chain may get stuck and take a very long time to converge on an
  # appropriate answer.
  jump_sd = 5,
  reps = 10000, # the length of our MCMC chain
  seed = sample.int(.Machine$integer.max, 1)
) {
  dat <- rnorm(.n, mean = mu, sd = sigma) # our simulated data

  # We will ensure that our data has exactly our specified mean.
  # (This is just to make the simulation easier to follow.)
  dat <- dat - (mean(dat) - mu)

  # A vector to hold our MCMC chain output:
  out <- vector(length = reps, mode = "numeric")

  # We'll start at an initial value of 0:
  out[1] <- 0

  # Now we'll loop through our MCMC chain:
  for (i in seq(2, length(out))) {

    # Propose a new value given the previous value and the jump SD:
    proposal <- rnorm(1, out[1], jump_sd)

    # Calculate the log-likelihood of the data given the proposed parameter value
    # and the previous parameter value:
    log_like_proposal <- sum(dnorm(dat, mean = proposal, sd = sigma, log = TRUE))
    log_like_previous <- sum(dnorm(dat, mean = out[i - 1], sd = sigma, log = TRUE))

    # Get the log-probability of the proposed and previous parameter values given
    # the prior:
    log_prior_proposal <- dnorm(proposal, mean = prior_mu, sd = prior_mu_sd, log = TRUE)
    log_prior_previous <- dnorm(out[i - 1], mean = prior_mu, sd = prior_mu_sd, log = TRUE)

    # Combine the log-prior with the log-likelihood to get a value that is
    # proportional to the posterior probability of that parameter value given the
    # data:
    log_posterior_proposal <- log_prior_proposal + log_like_proposal
    log_posterior_previous <- log_prior_previous + log_like_previous

    # Calculate the ratio of the proposal and previous probabilities:
    prob_ratio <- exp(log_posterior_proposal - log_posterior_previous)

    # If the probability ratio is > 1, then always accept the new parameter
    # values.
    # If the probability ratio is < 1, then accept the new parameter values in
    # proportion to the ratio.
    if (runif(1, min = 0, max = 1) < prob_ratio) {
      out[i] <- proposal # use the proposed parameter value
    } else {
      out[i] <- out[i - 1] # keep the previous parameter value
    }
  }

  par(mfrow = c(1, 3))
  plot(dat, main = "Observed data")
  plot(seq_along(out), out,
    type = "l", xlab = "Chain index",
    ylab = expression(widehat(mu)), col = "#00000050",
    main = "Traceplot"
  ) # the MCMC chain
  abline(h = mu, col = "red", lty = 1, lwd = 2) # the true mean
  hist(out,
    xlim = c(-2, 7), breaks = 30,
    main = "Prior (line)\nand Posterior (histogram)", xlab = "mu"
  )
  abline(v = mu, col = "red", lty = 1, lwd = 2)
  xx <- seq(-10, 10, length.out = 200)
  yy <- dnorm(xx, mean = prior_mu, sd = prior_mu_sd)
  par(new = TRUE)
  plot(xx, yy,
    ylim = c(0, max(yy)), type = "l", axes = FALSE, ann = FALSE,
    xlim = c(-5, 5)
  )
  invisible(out)
}
```

Now, we'll call our function, which by default will take 10,000 MCMC samples:

```{r}
mcmc_example()
```

We can play with our function using the manipulate package:

```{r, eval=FALSE}
library(manipulate)
manipulate(
  mcmc_example(
    mu = mu,
    sigma = sigma,
    .n = .n,
    prior_mu = 0,
    prior_mu_sd = 10,
    jump_sd = jump_sd,
    reps = reps,
    seed = 42
  ),
  mu = slider(0, 5, 1, step = 0.1),
  sigma = slider(0.1, 10, 5, step = 0.1),
  .n = slider(5, 1000, 25, step = 5),
  jump_sd = slider(0.05, 40, 5, step = 0.1),
  reps = slider(50, 20000, 1000, step = 50),
  seed = slider(1, 50, 42, step = 1)
)
```

# Questions:

1. What happens to the posterior as the number of data points (`.n`) gets large?

2. What happens to the posterior as the number of data points (`.n`) gets small?

3. What happens to the posterior as the observation error (`sigma`) gets larger?

4. What happens if the jump distance gets very big?

5. What happens if the jump distance gets very small?
