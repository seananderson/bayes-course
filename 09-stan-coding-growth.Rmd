---
title: "More coding in Stan: a non-linear model example"
output:
  html_document:
    toc: true
    toc_float: true
---

# Goals:

- Gain exposure to a non-linear model written in Stan that's more complicated than the simple linear regression.
- See how we can do prior predictive simulations in a custom Stan model.
- Gain an initial exposure to Bayesian model comparison.

# Setup

```{r, message=FALSE, warning=FALSE}
library(rstan)
library(dplyr)
library(ggplot2)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
show_file <- function(file) {
  cat(paste(readLines(file), collapse = "\n"))
}
```

# Exercise

These are Pacific Cod age and length data from two DFO synoptic trawl surveys in BC for Hecate Strait and West Coast Vancouver Island.

```{r}
d <- readRDS(here::here("data/pcod-growth.rds"))
d$i <- seq_len(nrow(d))
ggplot(d, aes(age, length)) + geom_point() +
  facet_wrap(~survey)
```

We will fit a von Bertalanffy growth model to these data. Here's a simple version:

```{r}
show_file("vb/vb_basic.stan")
```

We will assume normally distributed observation error for simplicity (it's also relatively common). 

Why is this technically not an ideal choice? 

What would be some alternatives?

This version has some more bells and whistles:

```{r}
show_file("vb/vb_norm.stan")
```

Form our data list for rstan:

```{r}
dat <- list(
  N = nrow(d),
  length = d$length,
  age = d$age,
  prior_sds = c(k = 1, linf = 200, t0 = 1, sigma = 10),
  prior_only = 0
)
```

Sample from our model:

```{r, results="hide", message=FALSE}
fit1 <- stan(here::here("vb/vb_norm.stan"), data = dat, iter = 1000, chains = 4, seed = 9129)
```

Note that we set the `seed`` here. Why might we want to do that?

Print the parameters of interest:

Why am I specifying the parameters explicitly here?

```{r}
print(fit1, pars = c("k", "linf", "t0", "sigma"))
```

There are many formats we can extract posterior samples in:

```{r}
sims_list <- extract(fit1)
names(sims_list)

sims_matrix <- as.matrix(fit1)
dim(sims_matrix)
colnames(sims_matrix)[1:4]

sims_array <- as.array(fit1)
dim(sims_array)
colnames(sims_array[1,,])[1:4]

sims_df <- as.data.frame(fit1)
head(sims_df[,1:4])
```

Look at MCMC trace plots:

```{r}
dim(sims_array)
bayesplot::mcmc_trace(sims_array[,,1:4])
```

Grab posterior predictions of new observations:

We'll use `tidybayes::gather_draws()`, which conveniently gathers 20 MCMC samples for `length_sim` and forms a "long-format" data frame that is easy to work with in dplyr or ggplot. `[i]` tells the function to index each observation as `i` in the data frame.

Again, I set the seed here. Why is that?

```{r}
post <- tidybayes::gather_draws(fit1, length_sim[i], ndraws = 20, seed = 9283)
head(post)

# column 'i' was previously added to our data above so we can join on it:
post <- left_join(post, d)
ggplot(post, aes(age, .value)) + geom_point(alpha = 0.2)
```

Question: what are we looking at?

Expected length values:

```{r}
post <- tidybayes::gather_draws(fit1, predicted_length[i], ndraws = 20, seed = 9283)
head(post)

post <- left_join(post, d)
ggplot(post, aes(age, .value, group = .draw)) + geom_line(alpha = 0.2) +
  geom_point(data = d, mapping = aes(age, length), inherit.aes = FALSE, alpha = 0.2)
```

Question: what are we looking at?

The parameter posteriors:

```{r}
post <- tidybayes::gather_draws(fit1, c(k, linf, t0, sigma))
head(post)
ggplot(post, aes(.value)) + geom_histogram() +
  facet_wrap(~.variable, scales = "free_x")
```

We can calculate ELPD (expected log predictive density) of a leave-one-out approximation because we included `log_lik` in our `generated quantities` section. It's not all that useful yet though, since we've only fit one model. We'll dive more deeply into this elsewhere.

```{r}
loo1 <- loo(fit1)
loo1
```

# Extending our model

We're now going to take the above model and modify it to allow for separate k, linf, and t0 by survey region (HS vs. WCVI). We want to ask whether there is evidence in the growth curve that we should be treating these regions as separate stocks.

```{r}
show_file("vb/vb_norm_regions.stan")
```

```{r, results="hide"}
dat2 <- dat
levels(factor(d$survey))
unique(as.integer(factor(d$survey)))
dat2$survey_id <- as.integer(factor(d$survey))
dat2$N_surveys <- 2

fit2 <- stan(here::here("vb/vb_norm_regions.stan"), data = dat2, iter = 1000, chains = 4, seed = 9129)
```

```{r}
print(fit2, pars = c("k", "linf", "t0", "sigma"))
```

```{r}
sims2 <- extract(fit2)
```

Plot the 80% quantile credible interval of expected values and the posterior predictive distribution:

```{r}
dim(sims2$length_sim)
d$upr <- apply(sims2$length_sim, 2, quantile, probs = 0.9)
d$med <- apply(sims2$length_sim, 2, quantile, probs = 0.5)
d$lwr <- apply(sims2$length_sim, 2, quantile, probs = 0.1)

d$e_upr <- apply(sims2$predicted_length, 2, quantile, probs = 0.9)
d$e_med <- apply(sims2$predicted_length, 2, quantile, probs = 0.5)
d$e_lwr <- apply(sims2$predicted_length, 2, quantile, probs = 0.1)

ggplot(d, aes(age, med, colour = survey, fill = survey)) + 
  geom_line() +
  geom_line(aes(y = e_med)) +
  geom_ribbon(aes(ymin = e_lwr, ymax = e_upr), alpha = 0.5, colour = NA) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.2, colour = NA) +
  geom_point(data = d, mapping = aes(age, length), inherit.aes = FALSE, alpha = 0.2) +
  ylab("Length") + xlab("Age")
```

```{r}
y <- d$length
sims1 <- extract(fit1)
sims2 <- extract(fit2)
set.seed(1)
i <- sample(1:nrow(sims1$length_sim), size = 20)
yrep1 <- sims1$length_sim[i,]
yrep2 <- sims2$length_sim[i,]
bayesplot::ppc_dens_overlay(y, yrep1)
bayesplot::ppc_dens_overlay(y, yrep2)

bayesplot::ppc_dens_overlay_grouped(y, yrep1, group = d$survey)
bayesplot::ppc_dens_overlay_grouped(y, yrep2, group = d$survey)
```

The `^`s in the following "regular expressions" simply mean the parameters must start with each of these patterns.
<https://xkcd.com/208/>

```{r}
sims_array2 <- as.array(fit2)[,,1:10]
bayesplot::mcmc_trace(sims_array2, regex_pars = c("^k", "^linf", "^t0", "^sigma"))
```

What does ELPD based on LOO cross validation tell us about the two models?

```{r}
loo2 <- loo(fit2)
loo::loo_compare(loo1, loo2)
```

We can compare the posterior distributions of parameters from the two surveys.

We will calculate differences for each of the parameters:

```{r}
sims <- extract(fit2)
dim(sims$k)
k_diff <- sims$k[,2] - sims$k[,1]
linf_diff <- sims$linf[,2] - sims$linf[,1]
t0_diff <- sims$t0[,2] - sims$t0[,1]
```

```{r}
hist(k_diff)
hist(linf_diff)
hist(t0_diff)
```

# Exercise:

Using those posterior draws, answer the following questions:

What is the probability that linf is greater in WCVI than in HS?

```{r}
mean(linf_diff > 0) # exercise
```

What is the probability that linf is at least 5 cm greater in WCVI than in HS?

```{r}
mean(linf_diff > 5) # exercise
```

What is the probability that linf in WCVI is different from linf in HS by more than 4cm?

```{r}
mean(linf_diff > 4 | linf_diff < -4)  # exercise
```
