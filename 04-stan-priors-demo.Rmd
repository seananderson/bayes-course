---
title: "Demo of the influence of priors in a basic Stan regression"
author: "Sean Anderson"
output: html_document
---

# Goals

- Gain some intuition about the influence of priors on a Bayesian regression

# Demo

First, run the following code chunk, which includes a function to simulate data and fit a regression using `rstanarm::stan_glm()`.

```{r, message=FALSE, warning=FALSE, results='hide'}
library(ggplot2)
library(rstanarm)
theme_set(theme_light())

mcmc_example <- function(
  seed = 1,
  intercept = 3,
  slope = 0.7,
  sigma = 2, # the true residual SD
  .n = 30, # number of data points to simulate
  prior_slope_mean = 0,
  prior_slope_sd = 3,
  prior_intercept_sd = 10,
  prior_aux_sd = 3,
  reps = 600 # the length of each MCMC chain
) {

  set.seed(seed)
  x <- arm::rescale(runif(.n, -2, 2))
  d <- data.frame(x = x, y = rnorm(.n, mean = intercept + slope * x, sd = sigma))

  m <- stan_glm(y ~ x, d, iter = reps, chains = 1,
    family = gaussian(link = "identity"), refresh = 0,
    prior = normal(prior_slope_mean, prior_slope_sd, autoscale = FALSE),
    prior_intercept = normal(0, prior_intercept_sd, autoscale = FALSE),
    prior_aux = normal(0, prior_aux_sd, autoscale = FALSE),
    chains = 1, cores = 1
    )

  e <- as.data.frame(m)

  xx <- seq(-6, 6, length.out = 200)

  slope_prior <- data.frame(x = xx,
    y = dnorm(xx, mean = prior_slope_mean, sd = prior_slope_sd))

  intercept_prior <- data.frame(x = xx,
    y = dnorm(xx, mean = 0, sd = prior_intercept_sd))

  xx0 <- seq(0, 6, length.out = 200)
  sigma_prior <-  data.frame(x = xx0,
    y = extraDistr::dhnorm(xx0, sigma = prior_aux_sd))

  .range <- c(-4, 4)

  g1 <- ggplot(e, aes(`(Intercept)`, after_stat(density))) + geom_histogram(bins = 50) +
    geom_line(data = intercept_prior, aes(x, y), col = "blue") +
    coord_cartesian(xlim = .range) +
    xlab("Intercept") +
    geom_vline(xintercept = intercept, col = "red")

  g2 <- ggplot(e, aes(x, after_stat(density))) + geom_histogram(bins = 50) +
    geom_line(data = slope_prior, aes(x, y), col = "blue") +
    coord_cartesian(xlim = .range) +
    xlab("Slope coefficient") +
    geom_vline(xintercept = slope, col = "red")

  g3 <- ggplot(e, aes(sigma, after_stat(density))) + geom_histogram(bins = 50) +
    geom_line(data = sigma_prior, aes(x, y), col = "blue") +
    coord_cartesian(xlim = c(0, max(.range))) +
    xlab("Observation error SD") +
    geom_vline(xintercept = sigma, col = "red")

  nd <- data.frame(x = seq(-1, 1, length.out = 2))
  pp <- posterior_linpred(m, newdata = nd, draws = 50)
  pp2 <- reshape2::melt(pp)
  pp2$x <- rep(nd$x, each = 50)

  g4 <- ggplot(d, aes(x = x, y = y)) +
    geom_point() +
    geom_line(data = pp2, aes(x, value, group = iterations), inherit.aes = FALSE,
      alpha = 0.5, col = "grey30") +
    geom_abline(slope = slope, intercept = intercept,
      col = "red")

  gridExtra::grid.arrange(g1, g2, g3, g4, ncol = 2)
}
```

We can run our function once with default argument values:

```{r}
mcmc_example()
```

In the plots, the gray histograms represent the posterior samples, the blue lines represent the priors, and the red lines represent the true values. The last panel represents the simulated data (dots), the true relationship between y and x (red line), and 50 draws from the posterior distribution. 

Try running the following code chunk and adjusting the argument values. If you adjust the `seed` slider, you will change the random number generator draw. Note that the single predictor, `x`, which could represent any continuous predictor, has been centered on its mean and divided by 2 times its standard deviation. Remember that the choice of scale or standard deviation value for the priors is dependent on the scale of the predictor and response variable. 

You may consider asking questions like:

- What happens if you have a tight prior on the slope coefficient that differs from the true value?
- How does this change when you have a lot of data vs. a little? 
- How much data do I need before the data overwhelm the prior for a given discrepancy between the prior and data?
- What happens with a very diffuse prior on the slope coefficient and very few data points? 
- Are there scenarios where it's helpful to have a weakly informative prior on the observation error standard deviation?
- Can having an informative prior on the slope coefficient help if you've collected very little data? When might you have this kind of information? 

```{r, eval=FALSE}
library(manipulate)
manipulate(
  mcmc_example(
    seed = seed,
    intercept = 2.4,
    slope = slope,
    sigma = sigma,
    .n = .n,
    prior_slope_mean = prior_slope_mean,
    prior_slope_sd = prior_slope_sd,
    prior_intercept_sd = prior_intercept_sd,
    prior_aux_sd = prior_aux_sd),
  seed = slider(1, 1000, 42, step = 1),
  # intercept = slider(-5, 5, 0, step = 0.2),
  slope = slider(-1.6, 1.6, 0.8, step = 0.2),
  sigma = slider(0.1, 8, 1, step = 0.1),
  .n = slider(2, 200, 30),
  prior_slope_mean = slider(-5, 5, 0, step = 0.5),
  prior_slope_sd = slider(0.2, 10, 2, step = 0.2),
  prior_intercept_sd = slider(0.2, 20, 10, step = 0.2),
  prior_aux_sd = slider(0.2, 10, 3, step = 0.2)
)
```
