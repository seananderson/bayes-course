---
title: Divergent transitions
---

Classic dataset and model...

<https://www.martinmodrak.cz/2018/02/19/taming-divergences-in-stan-models/>

this version of code adapted from:
<https://michael-franke.github.io/Bayesian-Regression/practice-sheets/05b-divergences.html>

see:
<https://statmodeling.stat.columbia.edu/2014/01/21/everything-need-know-bayesian-statistics-learned-eight-schools/>

Rubin 1981: <https://www.jstor.org/stable/1164617>

```{r}
library(rstan)
```

This dataset represents a test of a coaching program on SAT scores across 8 schools. `y` is the average improvement (or not) test score for a given school and `sigma` is the standard error on that average improvement. 

```{r}
dat <- list(
  N = 8,
  y = c(28, 8, -3, 7, -1, 1, 18, 12),
  sigma = c(15, 10, 16, 11, 9, 11, 10, 18)
)
```

The idea is we can fit a heirarchical model that partially pools information across schools. The model has a 'true' latent test score change per school with some variance that results in the observed values.

```{stan eval=FALSE}
data {
  int<lower=0> N; // number of schools
  vector[N] y; // mean score change by school
  vector<lower=0>[N] sigma; // standard error on score change by school
}
parameters {
  real mu; // across-school average improvement
  real<lower=0> sigma_prime; // SD of latent score values
  vector[N] theta; // latent score values by school
}
model {
  mu ~ normal(0, 10); // prior
  sigma_prime ~ cauchy(0, 10); // prior
  theta ~ normal(mu, sigma_prime); // latent score values
  y ~ normal(theta, sigma); // data likelihood
}
```

Fit the model with Stan:

```{r}
mod1 <- stan("stan/8schools.stan", data = dat, seed = 9283)
mod1
```

Check divergences:

```{r}
rstan::get_num_divergent(mod1)
```

```{r, eval=FALSE}
shinystan::launch_shinystan(mod1)
```

We can visualize what's going on. The sampler isn't going all the way down the 'funnel' of a the SD of the latent values `sigma_prime` and any specific school latent `theta`:

```{r}
bayesplot::mcmc_scatter(
  as.array(mod1),
  pars = c("theta[1]", "sigma_prime"),
  transform = list(sigma_prime = "log"),
  np = bayesplot::nuts_params(mod1)
)
```

Note the red dots clustered towards the bottom of the funnel.

We can instead fit a version that is parameterized differently:

```{stan, eval=FALSE}
data {
  int<lower=0> N;
  vector[N] y;
  vector<lower=0>[N] sigma;
}
parameters {
  real mu;
  real<lower=0> sigma_prime;
  vector[N] eta; // temporary variable: Normal(0, 1)
}
transformed parameters {
  vector[N] theta; // define theta before we use it
  // now we form `theta` here based on sigma_prime * eta:
  theta = mu + sigma_prime * eta;
}
model {
  mu ~ normal(0, 10);
  sigma_prime ~ cauchy(0, 10);
  eta ~ normal(0, 1); // here's our temporary Normal(0, 1) variable
  y ~ normal(theta, sigma);
}
```

Everything not commented is the same as before. We have a Normal(0, 1) variable `eta` that we multiply by `sigma_prime`. We have the same probability model, but now the sampler is tracking `sigma_prime` and `eta`, which are uncorrelated, so it has less of a problem exploring the full posterior.

```{r}
mod2 <- stan("stan/8schools_noncentered.stan", data = dat, seed = 9283)
```

```{r}
bayesplot::mcmc_scatter(
  as.array(mod2),
  pars = c("theta[1]", "sigma_prime"),
  transform = list(sigma_prime = "log"),
  np = bayesplot::nuts_params(mod2)
)
```

That's much better and out divergences do not appear to be systematically where we might worry about them (although that's harder to say with more complicated models). We can improve this further by increasing the `adapt_delta` from its default 0.8 towards 1:

```{r}
mod3 <- stan(
  "8schools_noncentered.stan", data = dat, seed = 9283, 
  control = list(adapt_delta = 0.95)
)
```

```{r}
bayesplot::mcmc_scatter(
  as.array(mod3),
  pars = c("theta[1]", "sigma_prime"),
  transform = list(sigma_prime = "log"),
  np = bayesplot::nuts_params(mod3)
)
```

We can check what effect ignoring those divergent transitions would have had on our estimate of `sigma_prime`:

```{r}
p1 <- extract(mod1)
p3 <- extract(mod3)
```

```{r}
par(mfrow = c(2, 1))
hist(log(p1$sigma_prime), xlim = c(-10, 5))
hist(log(p3$sigma_prime), xlim = c(-10, 5))

mean(log(p1$sigma_prime))
mean(log(p3$sigma_prime))
```

So, we would have lost the lower tail of the parameter if we had ignored the divergent transitions and ended up with some bias the parameter posterior.
