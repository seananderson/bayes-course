---
title: "An introduction to Stan programming"
output:
  html_document:
    toc: true
    toc_float: true
---

# Goals 

- Learn the basics of Stan model syntax and how to interact with Stan in R
- Learn to generate our own posterior predictions from a Stan model 
- Learn how to calculate LOOIC from a Stan model we wrote ourselves

# Setup 

```{r, echo=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.asp = 0.618,
  fig.align = "center"
)
show_file <- function(file) {
  cat(paste(readLines(file), collapse = "\n"))
}
```

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
theme_set(theme_light())
```

# Exercise

Let's load the rstan R package, which lets us interact with R.

As an aside, the cmdstanr is another option for interacting with Stan from R and has several advantages <https://mc-stan.org/cmdstanr/>.

```{r}
library(rstan)
```

If we wanted, we could set our option to use parallel processing. This would be useful with more complicated models or more data. For this simple exercise, it will probably be fastest with a single core. 

```{r}
# options(mc.cores = parallel::detectCores())
options(mc.cores = 1) # default
```

Another important option we can set with rstan is whether we want the compiled model to be saved so that we don't have to recompile it every time we run it. You probably want to set this because the compilation can take a while. 

```{r}
rstan_options(auto_write = TRUE)
```

# Simulating data

Let's simulate some data to fit a very simple Stan model to. We will focus on a linear model with normally distributed errors.

```{r}
set.seed(42)
N <- 30
x <- rnorm(N, 0, 0.5)
alpha <- -0.2
beta <- 0.4
sigma <- 0.3
y <- rnorm(N, mean = alpha + x * beta, sd = sigma)
dat <- tibble(x = x, y = y)
```

```{r}
ggplot(dat, aes(x, y)) + geom_point()
```

# The Stan model 

Take a look at the following Stan model that I have set up. 

We'll talk about the various sections as a group. 

```{r}
show_file("stan/lm-simple.stan")
```

This next version also has a generated quantity section. This is the one we will actually run. The generated quantities make it easier to do some things after like make posterior predictions. 

We'll talk about these additions as a group.

```{r}
show_file("stan/lm.stan")
```

# Fitting the model

The first time we run the next code chunk, Stan will translate our model into C++ and compile it. This will take a little while. After that, assuming we set `rstan_options(auto_write = TRUE)`, Stan will avoid recompiling the model unless something in the model code changes.

Let's sample from the model now:

```{r, message=FALSE, results='hide'}
fit <- stan(here::here("stan/lm.stan"), chains = 4, iter = 2000, seed = 39382,
  data = list(x = dat$x, y = dat$y, N = length(dat$y)))
```

Congratulations --- you fit your first handwritten Stan model! We can do everything with the posterior samples that we could do with the samples from an rstanarm model or a brms model.

Some of the built-in helper functions from those packages won't work with our model though.

```{r, eval=FALSE}
fit
```

Notice all of the extra lines of reported results for our generated quantities? We can focus just on the parameters we want with:

```{r}
pars <- c("alpha", "beta", "sigma")
print(fit, pars = pars)
```

We can use shinystan if we want to look at the model. 

```{r, eval=FALSE}
shinystan::launch_shinystan(fit)
```

The default plot method shows point estimates and credible intervals.

```{r}
plot(fit, pars = pars)
```

There are other options. 

```{r, eval=FALSE}
?rstan::`rstan-plotting-functions`
stan_dens(fit, pars = pars)
```

Alternatively, and probably preferably, we can use the bayesplot package. 

Experiment with inspecting the posterior chains using the bayesplot package.

```{r}
fit_array <- as.array(fit)
bayesplot::mcmc_trace(fit_array, pars = pars) 
bayesplot::mcmc_dens_overlay(fit_array, pars = pars) # exercise
```

# Manipulating the posterior samples ourselves

```{r}
post <- rstan::extract(fit)
```

The output from `rstan::extract()` is a named list. Each element of the list is a numeric vector of samples if that parameter had one dimension (as all of our parameters did this time), or a matrix of samples if, say, beta had represented multiple slope parameters. 

```{r}
names(post)
dim(post$beta)
```

Let's overlay some model fits from the posterior on the data:

```{r}
N <- 200
ggplot(dat, aes(x, y)) + geom_point() +
  geom_abline(
    intercept = post$alpha[1:N], 
    slope = post$beta[1:N], 
    alpha = 0.1)
```

Is that starting to look like the confidence/credible intervals you are used to looking at?

# Posterior predictive simulations

We can experiment with inspecting the posterior predictive distribution using the bayesplot package:

```{r}
post <- rstan::extract(fit) # extract the posterior simulations as a list
# grab the `posterior_predictions` from our `generated quantities` section:
pp <- post$posterior_predictions
bayesplot::ppc_dens_overlay(y = dat$y, yrep = pp[1:25, ])
```

Let's do it in R to see how we could create them after. The result is the same. The question is whether it is simpler to compute the posterior simulations with in the Stan code or in R.

We need to form the prediction for each data point and add observation error. Let's manually create 8 draws from the posterior predictive distribution and compare them to our data. 

There are many ways you could do this. The following is one way. We will create a list with posterior predictions within data frames and then bind the elements of the list into a big data frame. We are doing this so it is easy to plot the output with ggplot.

```{r}
set.seed(1)
n_sim <- 8
out <- list()
for (i in seq_len(n_sim)) {
  out[[i]] <- tibble(x = x)
  out[[i]]$i <- i
  out[[i]]$y_pp <-
    rnorm(
      n = length(x),
      mean = post$alpha[i] + post$beta[i] * x,
      sd = post$sigma[i]
    )
}
out <- dplyr::bind_rows(out) # turn the list into a data.frame
out$type <- "posterior prediction"

# add the real data as the last panel:
out <- dplyr::bind_rows(
  out,
  tibble(x = x, y_pp = y, type = "observed", i = 9)
)

ggplot(out, aes(x, y_pp, colour = type)) +
  geom_point() +
  facet_wrap(~i)
```

## Questions:

Which approach do you prefer in this case (Stan generated quantities vs. R code)? 

When might you prefer one over the other approach?
